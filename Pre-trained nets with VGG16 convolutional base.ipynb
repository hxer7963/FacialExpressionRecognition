{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fine-tunning with VGG16 convolutional base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 48, 48, 3)         6         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 15,247,181\n",
      "Trainable params: 532,493\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# pretrain the FC layer then to fine-tunning the vgg16 conv_base net\n",
    "import os\n",
    "import Utils\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "conv_base = VGG16(include_top=False, weights='imagenet', input_shape=(48, 48, 3))\n",
    "\n",
    "model = models.Sequential()\n",
    "# convert one channel to three without preprocess the grayscale to colored image.\n",
    "model.add(layers.Conv2D(3, kernel_size=(1, 1), input_shape=(48, 48, 1), activation='relu'))\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "conv_base.trainable = False  # fixed the convolution base before fine-tunning the end-to-end network\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "train_generator = Utils.image_data_generator(Utils.train_dir)\n",
    "validation_generator = Utils.image_data_generator(Utils.validation_dir)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer=optimizers.RMSprop(lr=1e-4, decay=1e-6),\n",
    "            metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  note: before train 20 epoch, there is 70 epoch pre-train with the same net architeture, but the model seem still underfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.7378 - acc: 0.2960 - val_loss: 1.6364 - val_acc: 0.3662\n",
      "Epoch 2/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.6484 - acc: 0.3495 - val_loss: 1.6079 - val_acc: 0.3779\n",
      "Epoch 3/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.6149 - acc: 0.3681 - val_loss: 1.5920 - val_acc: 0.3860\n",
      "Epoch 4/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.5970 - acc: 0.3768 - val_loss: 1.5803 - val_acc: 0.3927\n",
      "Epoch 5/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.5781 - acc: 0.3863 - val_loss: 1.5768 - val_acc: 0.3902\n",
      "Epoch 6/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.5647 - acc: 0.3946 - val_loss: 1.5659 - val_acc: 0.3933\n",
      "Epoch 7/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.5581 - acc: 0.3988 - val_loss: 1.5673 - val_acc: 0.3941\n",
      "Epoch 8/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.5407 - acc: 0.4065 - val_loss: 1.5612 - val_acc: 0.3947\n",
      "Epoch 9/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.5430 - acc: 0.4078 - val_loss: 1.5631 - val_acc: 0.3936\n",
      "Epoch 10/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.5285 - acc: 0.4114 - val_loss: 1.5584 - val_acc: 0.4006\n",
      "Epoch 11/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.5112 - acc: 0.4228 - val_loss: 1.5538 - val_acc: 0.4034\n",
      "Epoch 12/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.5081 - acc: 0.4238 - val_loss: 1.5528 - val_acc: 0.4025\n",
      "Epoch 13/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.5032 - acc: 0.4241 - val_loss: 1.5497 - val_acc: 0.4034\n",
      "Epoch 14/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.4991 - acc: 0.4297 - val_loss: 1.5532 - val_acc: 0.4039\n",
      "Epoch 15/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.4891 - acc: 0.4318 - val_loss: 1.5483 - val_acc: 0.4075\n",
      "Epoch 16/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.4689 - acc: 0.4432 - val_loss: 1.5487 - val_acc: 0.4070\n",
      "Epoch 17/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.4739 - acc: 0.4399 - val_loss: 1.5590 - val_acc: 0.4031\n",
      "Epoch 18/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.4610 - acc: 0.4463 - val_loss: 1.5513 - val_acc: 0.4103\n",
      "Epoch 19/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.4627 - acc: 0.4482 - val_loss: 1.5556 - val_acc: 0.4123\n",
      "Epoch 20/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.4523 - acc: 0.4549 - val_loss: 1.5506 - val_acc: 0.4145\n",
      "Epoch 21/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.4467 - acc: 0.4576 - val_loss: 1.5506 - val_acc: 0.4196\n",
      "Epoch 22/80\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 1.4424 - acc: 0.4544 - val_loss: 1.5470 - val_acc: 0.4165\n",
      "Epoch 23/80\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 1.4317 - acc: 0.4638 - val_loss: 1.5508 - val_acc: 0.4184\n",
      "Epoch 24/80\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 1.4230 - acc: 0.4628 - val_loss: 1.5596 - val_acc: 0.4179\n",
      "Epoch 25/80\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 1.4244 - acc: 0.4681 - val_loss: 1.5539 - val_acc: 0.4254\n",
      "Epoch 26/80\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 1.4133 - acc: 0.4756 - val_loss: 1.5507 - val_acc: 0.4257\n",
      "Epoch 27/80\n",
      "1400/1400 [==============================] - 71s 50ms/step - loss: 1.4106 - acc: 0.4748 - val_loss: 1.5498 - val_acc: 0.4229\n",
      "Epoch 28/80\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 1.4065 - acc: 0.4780 - val_loss: 1.5512 - val_acc: 0.4257\n",
      "Epoch 29/80\n",
      "1400/1400 [==============================] - 70s 50ms/step - loss: 1.3986 - acc: 0.4847 - val_loss: 1.5600 - val_acc: 0.4291\n",
      "Epoch 30/80\n",
      "1400/1400 [==============================] - 70s 50ms/step - loss: 1.3887 - acc: 0.4819 - val_loss: 1.5574 - val_acc: 0.4232\n",
      "Epoch 31/80\n",
      "1400/1400 [==============================] - 71s 51ms/step - loss: 1.3909 - acc: 0.4837 - val_loss: 1.5539 - val_acc: 0.4263\n",
      "Epoch 32/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3888 - acc: 0.4897 - val_loss: 1.5525 - val_acc: 0.4304\n",
      "Epoch 33/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3785 - acc: 0.4924 - val_loss: 1.5609 - val_acc: 0.4318\n",
      "Epoch 34/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3748 - acc: 0.4961 - val_loss: 1.5590 - val_acc: 0.4327\n",
      "Epoch 35/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3678 - acc: 0.4982 - val_loss: 1.5601 - val_acc: 0.4360\n",
      "Epoch 36/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3650 - acc: 0.5003 - val_loss: 1.5684 - val_acc: 0.4346\n",
      "Epoch 37/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3586 - acc: 0.5017 - val_loss: 1.5696 - val_acc: 0.4332\n",
      "Epoch 38/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3539 - acc: 0.5043 - val_loss: 1.5686 - val_acc: 0.4352\n",
      "Epoch 39/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3489 - acc: 0.5082 - val_loss: 1.5627 - val_acc: 0.4346\n",
      "Epoch 40/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3475 - acc: 0.5115 - val_loss: 1.5666 - val_acc: 0.4366\n",
      "Epoch 41/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3407 - acc: 0.5130 - val_loss: 1.5640 - val_acc: 0.4388\n",
      "Epoch 42/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3354 - acc: 0.5130 - val_loss: 1.5718 - val_acc: 0.4419\n",
      "Epoch 43/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3304 - acc: 0.5148 - val_loss: 1.5776 - val_acc: 0.4352\n",
      "Epoch 44/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3278 - acc: 0.5213 - val_loss: 1.5710 - val_acc: 0.4433\n",
      "Epoch 45/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3186 - acc: 0.5222 - val_loss: 1.5777 - val_acc: 0.4360\n",
      "Epoch 46/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3147 - acc: 0.5244 - val_loss: 1.5757 - val_acc: 0.4408\n",
      "Epoch 47/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3136 - acc: 0.5244 - val_loss: 1.5788 - val_acc: 0.4335\n",
      "Epoch 48/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.3064 - acc: 0.5283 - val_loss: 1.5790 - val_acc: 0.4397\n",
      "Epoch 49/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3046 - acc: 0.5259 - val_loss: 1.5849 - val_acc: 0.4402\n",
      "Epoch 50/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.3028 - acc: 0.5317 - val_loss: 1.5834 - val_acc: 0.4475\n",
      "Epoch 51/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.2993 - acc: 0.5336 - val_loss: 1.5960 - val_acc: 0.4425\n",
      "Epoch 52/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2920 - acc: 0.5377 - val_loss: 1.5824 - val_acc: 0.4447\n",
      "Epoch 53/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.2818 - acc: 0.5412 - val_loss: 1.5944 - val_acc: 0.4436\n",
      "Epoch 54/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2810 - acc: 0.5402 - val_loss: 1.5881 - val_acc: 0.4461\n",
      "Epoch 55/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2788 - acc: 0.5398 - val_loss: 1.5969 - val_acc: 0.4464\n",
      "Epoch 56/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2798 - acc: 0.5425 - val_loss: 1.5876 - val_acc: 0.4475\n",
      "Epoch 57/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2683 - acc: 0.5467 - val_loss: 1.5970 - val_acc: 0.4489\n",
      "Epoch 58/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2641 - acc: 0.5467 - val_loss: 1.6077 - val_acc: 0.4469\n",
      "Epoch 59/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2666 - acc: 0.5479 - val_loss: 1.6012 - val_acc: 0.4458\n",
      "Epoch 60/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2564 - acc: 0.5564 - val_loss: 1.5948 - val_acc: 0.4511\n",
      "Epoch 61/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2541 - acc: 0.5550 - val_loss: 1.6090 - val_acc: 0.4455\n",
      "Epoch 62/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2514 - acc: 0.5564 - val_loss: 1.6025 - val_acc: 0.4453\n",
      "Epoch 63/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.2489 - acc: 0.5561 - val_loss: 1.6199 - val_acc: 0.4511\n",
      "Epoch 64/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2455 - acc: 0.5594 - val_loss: 1.6218 - val_acc: 0.4517\n",
      "Epoch 65/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2354 - acc: 0.5631 - val_loss: 1.6168 - val_acc: 0.4503\n",
      "Epoch 66/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2390 - acc: 0.5623 - val_loss: 1.6106 - val_acc: 0.4511\n",
      "Epoch 67/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2346 - acc: 0.5681 - val_loss: 1.6155 - val_acc: 0.4503\n",
      "Epoch 68/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2349 - acc: 0.5653 - val_loss: 1.6247 - val_acc: 0.4486\n",
      "Epoch 69/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.2175 - acc: 0.5722 - val_loss: 1.6277 - val_acc: 0.4584\n",
      "Epoch 70/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.2240 - acc: 0.5705 - val_loss: 1.6304 - val_acc: 0.4492\n",
      "Epoch 71/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2168 - acc: 0.5722 - val_loss: 1.6241 - val_acc: 0.4464\n",
      "Epoch 72/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2123 - acc: 0.5767 - val_loss: 1.6299 - val_acc: 0.4553\n",
      "Epoch 73/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2161 - acc: 0.5728 - val_loss: 1.6412 - val_acc: 0.4486\n",
      "Epoch 74/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2037 - acc: 0.5763 - val_loss: 1.6334 - val_acc: 0.4503\n",
      "Epoch 75/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2056 - acc: 0.5794 - val_loss: 1.6394 - val_acc: 0.4506\n",
      "Epoch 76/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.1964 - acc: 0.5808 - val_loss: 1.6490 - val_acc: 0.4500\n",
      "Epoch 77/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.2019 - acc: 0.5760 - val_loss: 1.6447 - val_acc: 0.4497\n",
      "Epoch 78/80\n",
      "1400/1400 [==============================] - 74s 53ms/step - loss: 1.1946 - acc: 0.5858 - val_loss: 1.6527 - val_acc: 0.4461\n",
      "Epoch 79/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.1949 - acc: 0.5857 - val_loss: 1.6640 - val_acc: 0.4497\n",
      "Epoch 80/80\n",
      "1400/1400 [==============================] - 73s 52ms/step - loss: 1.1877 - acc: 0.5869 - val_loss: 1.6654 - val_acc: 0.4506\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "                            train_generator,\n",
    "                            steps_per_epoch=1400,\n",
    "                            epochs=80,\n",
    "                            validation_data=validation_generator,\n",
    "                            validation_steps=179)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3589 images belonging to 7 classes.\n",
      "end_to_end_before_fine_tunning_with_VGG16_pretrain_net_hit_44.61%\n"
     ]
    }
   ],
   "source": [
    "score = Utils.evaluate_model(model)\n",
    "print('end_to_end_before_fine_tunning_with_VGG16_pretrain_net_hit_%2.2f%%' % (score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Utils.save_model(model, 'end_to_end_before_fine_tunning_vgg16_70epochs_hit(48.23%).h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-10158d065657>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplt_acc_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end2end_before_fine_tunning_vgg16_accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end2end_before_fine_tunning_vgg16_loss'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "Utils.plt_acc_loss(history, 'end2end_before_fine_tunning_vgg16_accuracy', 'end2end_before_fine_tunning_vgg16_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 48, 48, 3)         6         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 15,247,181\n",
      "Trainable params: 7,611,917\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "L:\\Anaconda3\\lib\\site-packages\\keras\\models.py:291: UserWarning: Error in loading the saved optimizer state. As a result, your model is starting with a freshly initialized optimizer.\n",
      "  warnings.warn('Error in loading the saved optimizer '\n"
     ]
    }
   ],
   "source": [
    "# note: you can directely train the model rather than load a better before fine tuning model\n",
    "# here it's just to demostrate the process of fine-tuning VGG16.\n",
    "import Utils\n",
    "model = Utils.load_model('end_to_end_before_fine_tunning_vgg16hit(50.77%).h5')\n",
    "model.summary()\n",
    "conv_base = model.layers[1]\n",
    "# for layer in conv_base.layers:\n",
    "#   print(layer.name, layer.trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 48, 48, 3)         6         \n",
      "_________________________________________________________________\n",
      "vgg16 (Model)                (None, 1, 1, 512)         14714688  \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 1024)              525312    \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 7)                 7175      \n",
      "=================================================================\n",
      "Total params: 15,247,181\n",
      "Trainable params: 5,252,109\n",
      "Non-trainable params: 9,995,072\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "conv_base.trainable = True\n",
    "set_trainable = False\n",
    "for layer in conv_base.layers:\n",
    "    if layer.name == 'block5_conv1':\n",
    "        set_trainable = True\n",
    "    layer.trainable = set_trainable\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 28709 images belonging to 7 classes.\n",
      "Found 3589 images belonging to 7 classes.\n",
      "Epoch 1/20\n",
      "1400/1400 [==============================] - 87s 62ms/step - loss: 2.2774 - acc: 0.4385 - val_loss: 1.4274 - val_acc: 0.4656\n",
      "Epoch 2/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 1.2450 - acc: 0.5428 - val_loss: 1.3506 - val_acc: 0.5151\n",
      "Epoch 3/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 1.1139 - acc: 0.5973 - val_loss: 1.4104 - val_acc: 0.5209\n",
      "Epoch 4/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 1.0089 - acc: 0.6407 - val_loss: 1.5631 - val_acc: 0.4796\n",
      "Epoch 5/20\n",
      "1400/1400 [==============================] - 86s 62ms/step - loss: 0.9240 - acc: 0.6792 - val_loss: 1.7178 - val_acc: 0.5232\n",
      "Epoch 6/20\n",
      "1400/1400 [==============================] - 86s 62ms/step - loss: 0.8342 - acc: 0.7138 - val_loss: 1.7121 - val_acc: 0.5229\n",
      "Epoch 7/20\n",
      "1400/1400 [==============================] - 86s 62ms/step - loss: 0.7651 - acc: 0.7420 - val_loss: 2.2757 - val_acc: 0.4997\n",
      "Epoch 8/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 0.6951 - acc: 0.7700 - val_loss: 2.0957 - val_acc: 0.5162\n",
      "Epoch 9/20\n",
      "1400/1400 [==============================] - 86s 62ms/step - loss: 0.6312 - acc: 0.7957 - val_loss: 2.0775 - val_acc: 0.5394\n",
      "Epoch 10/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 0.5924 - acc: 0.8127 - val_loss: 1.9136 - val_acc: 0.5184\n",
      "Epoch 11/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 0.5343 - acc: 0.8361 - val_loss: 2.2188 - val_acc: 0.5453\n",
      "Epoch 12/20\n",
      "1400/1400 [==============================] - 85s 61ms/step - loss: 0.4926 - acc: 0.8469 - val_loss: 2.3859 - val_acc: 0.5229\n",
      "Epoch 13/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 0.4825 - acc: 0.8619 - val_loss: 2.5835 - val_acc: 0.5436\n",
      "Epoch 14/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 0.4554 - acc: 0.8695 - val_loss: 2.5194 - val_acc: 0.5380\n",
      "Epoch 15/20\n",
      "1400/1400 [==============================] - 86s 61ms/step - loss: 0.4152 - acc: 0.8810 - val_loss: 2.3415 - val_acc: 0.5553\n",
      "Epoch 16/20\n",
      "1400/1400 [==============================] - 86s 62ms/step - loss: 0.3918 - acc: 0.8899 - val_loss: 3.0488 - val_acc: 0.4729\n",
      "Epoch 17/20\n",
      "1400/1400 [==============================] - 86s 62ms/step - loss: 0.3790 - acc: 0.8980 - val_loss: 2.6448 - val_acc: 0.5399\n",
      "Epoch 18/20\n",
      "1400/1400 [==============================] - 82s 58ms/step - loss: 0.3618 - acc: 0.9037 - val_loss: 2.6795 - val_acc: 0.5338\n",
      "Epoch 19/20\n",
      "1400/1400 [==============================] - 83s 59ms/step - loss: 0.3574 - acc: 0.9089 - val_loss: 3.2472 - val_acc: 0.5293\n",
      "Epoch 20/20\n",
      "1400/1400 [==============================] - 85s 61ms/step - loss: 0.3424 - acc: 0.9118 - val_loss: 3.2740 - val_acc: 0.5084\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "train_generator = Utils.image_data_generator(Utils.train_dir)\n",
    "validation_generator = Utils.image_data_generator(Utils.validation_dir)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer=optimizers.RMSprop(lr=1e-4, decay=1e-6),\n",
    "             metrics=['acc'])\n",
    "\n",
    "\n",
    "hist = model.fit_generator(train_generator,\n",
    "                           steps_per_epoch=1400,\n",
    "                           epochs=20,\n",
    "                           validation_data=validation_generator,\n",
    "                           validation_steps=179)                           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3589 images belonging to 7 classes.\n",
      "svm on the top of pretrain VGG16 hit 55.06% accuracy\n"
     ]
    }
   ],
   "source": [
    "score = Utils.evaluate_model(model)\n",
    "print('svm on the top of pretrain VGG16 hit %.2f%% accuracy' % (score[1] * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "unsupported format character ')' (0x29) at index 43",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-fa8f736f2921>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mUtils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fine-tuning VGG16(100epoch_before_hit46.89%) 30epoch hit %.2f%%.h5'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: unsupported format character ')' (0x29) at index 43"
     ]
    }
   ],
   "source": [
    "Utils.save_model(model, 'fine-tuning VGG16(100epoch_before_hit46.89%) 30epoch hit %.2f%%.h5' % (score[1] * 100))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
